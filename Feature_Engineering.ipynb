{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5187c8a",
   "metadata": {},
   "source": [
    "### 1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3714eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b5130",
   "metadata": {},
   "source": [
    "### 2. Data Transformation & Feature Engineering with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f5763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transactions(spark: SparkSession, trx_results: list[dict]) -> DataFrame:\n",
    "\t\t\"\"\"\n",
    "\t\tSpark DataFrame with engineered features of Ethereum transactions.\n",
    "\t\t\"\"\"\n",
    "    # 1. Create Spark DataFrame\n",
    "    df = spark.createDataFrame(trx_results)\n",
    "    \n",
    "    # 2. Preprocessing\n",
    "    # Check Schema Structure \n",
    "\t  df.printSchema()\n",
    "    \n",
    "    # Transform 'contractAddress' column \n",
    "    df = df.withColumn(\"is_contract_call\", when(col(\"contractAddress\") != \"\", 1).otherwise(0))\n",
    "    \n",
    "    # Clean 'functionName' column data by removing () content \n",
    "    df = df.withColumn(\"functionName\", split(col(\"functionName\"), \"\\\\(\")[0])\n",
    "    \n",
    "    # Cast numeric columns\n",
    "    numeric_cols = ['blockNumber', 'timeStamp', 'nonce', 'transactionIndex', 'value', 'gas', 'gasPrice',\n",
    "                    'cumulativeGasUsed', 'gasUsed', 'confirmations', 'isError', 'txreceipt_status']\n",
    "    for c in numeric_cols:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, col(c).cast(DoubleType()))\n",
    "            \n",
    "    # Cast categorical columns\n",
    "    cat_cols = ['from', 'to', 'methodId', 'functionName']\n",
    "    for c in cat_cols:\n",
    "        if c in df.columns:\n",
    "            df = df.withColumn(c, col(c).cast(StringType()))\n",
    "    \n",
    "    # 3. Feature engineering \n",
    "    # 3-1. Temporal Feature \n",
    "\t  # timestamp -> datetime (UTC), date, week_start, hour \n",
    "\t  df = df.withColumn(\"datetime\", col(\"timeStamp\").cast(\"timestamp\"))\n",
    "\t\tdf = df.withColumn(\"date\", to_date(col(\"datetime\")))\n",
    "\t\tdf = df.withColumn(\"week_start\", date_trunc(\"week\", col(\"datetime\")))\n",
    "\t\tdf = df.withColumn(\"hour\", hour(col(\"datetime\")))\n",
    "   \n",
    "    # time-of-day slot\n",
    "    df = df.withColumn(\"time_slot\",\n",
    "    when((col(\"hour\") >= 0) & (col(\"hour\") < 6), \"midnight\")\n",
    "    .when((col(\"hour\") >= 6) & (col(\"hour\") < 12), \"morning\")\n",
    "    .when((col(\"hour\") >= 12) & (col(\"hour\") < 18), \"afternoon\")\n",
    "    .otherwise(\"night\"))\n",
    "\n",
    "    # transaction count and Fail rate \n",
    "    # Total \n",
    "    total_from = df.groupBy(\"from\").agg(\n",
    "        count(\"*\").alias(\"total_tx_count_from\"),\n",
    "        sum(col(\"isError\").cast(\"int\")).alias(\"total_fail_count_from\"))\n",
    "    total_from = total_from.withColumn(\"total_fail_rate_from\", (col(\"total_fail_count_from\")+1)/(col(\"total_tx_count_from\")+1)) #smoothing with +1\n",
    "    total_to = df.groupBy(\"to\").agg(\n",
    "        count(\"*\").alias(\"total_tx_count_to\"),\n",
    "        sum(col(\"isError\").cast(\"int\")).alias(\"total_fail_count_to\"))\n",
    "    total_to = total_to.withColumn(\"total_fail_rate_to\", (col(\"total_fail_count_to\")+1)/(col(\"total_tx_count_to\")+1)) # smoothing with +1\n",
    "    # Daily \n",
    "    daily_from = df.groupBy(\"from\", \"date\").agg(\n",
    "        count(\"*\").alias(\"daily_tx_count_from\"),\n",
    "        sum(col(\"isError\").cast(\"int\")).alias(\"daily_fail_count_from\"))\n",
    "    daily_from = daily_from.withColumn(\"daily_fail_rate_from\", (col(\"daily_fail_count_from\")+1)/(col(\"daily_tx_count_from\")+1)) \n",
    "    \n",
    "    daily_to = df.groupBy(\"to\", \"date\").agg(\n",
    "        count(\"*\").alias(\"daily_tx_count_to\"),\n",
    "        sum(col(\"isError\").cast(\"int\")).alias(\"daily_fail_count_to\"))\n",
    "    daily_to = daily_to.withColumn(\"daily_fail_rate_to\", (col(\"daily_fail_count_to\")+1)/(col(\"daily_tx_count_to\")+1)) \n",
    "    # Weekly \n",
    "    weekly_from = df.groupBy(\"from\", \"week_start\").agg(\n",
    "        count(\"*\").alias(\"weekly_tx_count_from\"),\n",
    "        sum(col(\"isError\").cast(\"int\")).alias(\"weekly_fail_count_from\"))\n",
    "    weekly_from = weekly_from.withColumn(\"weekly_fail_rate_from\", (col(\"weekly_fail_count_from\")+1)/(col(\"weekly_tx_count_from\")+1))\n",
    "\t  weekly_to = df.groupBy(\"to\", \"week_start\").agg(\n",
    "        count(\"*\").alias(\"weekly_tx_count_to\"),\n",
    "        sum(col(\"isError\").cast(\"int\")).alias(\"weekly_fail_count_to\"))\n",
    "    weekly_to = weekly_to.withColumn(\"weekly_fail_rate_to\", (col(\"weekly_fail_count_to\")+1)/(col(\"weekly_tx_count_to\")+1))\n",
    "    \n",
    "    # Daily average transaction interval\n",
    "\t\twindowSpec_from = Window.partitionBy(\"from\").orderBy(\"datetime\")\n",
    "\t\tdf = df.withColumn(\"prev_datetime_from\", lag(\"datetime\").over(windowSpec_from))\n",
    "\t\tdf = df.withColumn(\"interval_from\", \n",
    "\t\t    (col(\"datetime\").cast(\"long\") - col(\"prev_datetime_from\").cast(\"long\")).cast(DoubleType()))\n",
    "\n",
    "\t\twindowSpec_to = Window.partitionBy(\"to\").orderBy(\"datetime\")\n",
    "\t\tdf = df.withColumn(\"prev_datetime_to\", lag(\"datetime\").over(windowSpec_to))\n",
    "\t\tdf = df.withColumn(\"interval_to\", \n",
    "\t\t    (col(\"datetime\").cast(\"long\") - col(\"prev_datetime_to\").cast(\"long\")).cast(DoubleType()))\n",
    "\t\t\n",
    "\t\tdaily_avg_interval_from = (\n",
    "    df.groupBy(\"from\", \"date\")\n",
    "      .agg(mean(\"interval_from\").alias(\"daily_avg_interval_from\")))\n",
    "      \n",
    "    daily_avg_interval_to = (\n",
    "    df.groupBy(\"to\", \"date\")\n",
    "      .agg(mean(\"interval_to\").alias(\"daily_avg_interval_to\")))\n",
    "    \n",
    "    # 3-2. Value Feature \n",
    "    # Log transform \n",
    "    df = df.withColumn(\"log_value\", log1p(\"value\"))\n",
    "    df = df.withColumn(\"log_gasPrice\", log1p(\"gasPrice\"))\n",
    "    \n",
    "    # Spike detection (3 stddev above rolling mean)\n",
    "    rolling_window = Window.partitionBy(\"from\").orderBy(col(\"datetime\").cast(\"long\")).rangeBetween(-7*86400, -1)\n",
    "\n",
    "    df = df.withColumn(\"rolling_value_mean\", mean(\"log_value\").over(rolling_window))\n",
    "    df = df.withColumn(\"rolling_value_std\", stddev(\"log_value\").over(rolling_window))\n",
    "    df = df.withColumn(\"rolling_gasPrice_mean\", mean(\"log_gasPrice\").over(rolling_window))\n",
    "    df = df.withColumn(\"rolling_gasPrice_std\", stddev(\"log_gasPrice\").over(rolling_window))\n",
    "    df = df.withColumn(\"rolling_tx_count\", count(\"*\").over(rolling_window)) \n",
    "    \n",
    "    df = df.withColumn(\"value_zscore\",\n",
    "    (col(\"log_value\") - col(\"rolling_value_mean\")) / col(\"rolling_value_std\"))\n",
    "\t\tdf = df.withColumn(\"gasPrice_zscore\",\n",
    "    (col(\"log_gasPrice\") - col(\"rolling_gasPrice_mean\")) / col(\"rolling_gasPrice_std\"))\n",
    "    \n",
    "    df = df.withColumn(\"value_spike\",\n",
    "    when(col(\"rolling_tx_count\") < 10, 0)  # Not enough data\n",
    "    .when(col(\"rolling_value_std\").isNull(), 0)  # No stddev\n",
    "    .when(col(\"value_zscore\") > 3, 1)  # Strong spike\n",
    "    .otherwise(0))\n",
    "\n",
    "\t\tdf = df.withColumn(\"gasPrice_spike\",\n",
    "\t\t    when(col(\"rolling_tx_count\") < 10, 0)\n",
    "\t\t    .when(col(\"rolling_gasPrice_std\").isNull(), 0)\n",
    "\t\t    .when(col(\"gasPrice_zscore\") > 3, 1)\n",
    "\t\t    .otherwise(0))\n",
    "    \n",
    "    # 4. Encoding \n",
    "    \n",
    "    # Encoding coloums\n",
    "\t\tencoding_cols = ['methodId', 'functionName', 'time_slot']\n",
    "\n",
    "\t\t# StringIndexer (string -> number) \n",
    "\t\tindexers = [StringIndexer(inputCol=col, outputCol=col+\"_index\", handleInvalid=\"keep\")\n",
    "    for col in encoding_cols]\n",
    "\n",
    "\t\t# OneHotEncoder (Number â†’ one hot vector)\n",
    "\t\tencoders = [OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_onehot\")\n",
    "    for col in encoding_cols]\n",
    "\n",
    "\t\t# Pipeline execution \n",
    "\t\tpipeline = Pipeline(stages=indexers + encoders)\n",
    "\t\tmodel = pipeline.fit(df)\n",
    "\t\tdf_encoded = model.transform(df)\n",
    "\t\t\n",
    "\t\t# Results \n",
    "\t\tdf_encoded.select(\"methodId\", \"methodId_index\", \"methodId_onehot\").show(5, truncate=False)\n",
    "\t\t\n",
    "\t\t# Join all features back to main df\n",
    "    trx_df = df_encoded.join(daily_from, [\"from\",\"date\"], \"left\")\n",
    "    trx_df = df_encoded.join(daily_to, [\"to\",\"date\"], \"left\")\n",
    "    trx_df = df_encoded.join(weekly_from, [\"from\",\"week_start\"], \"left\")\n",
    "    trx_df = df_encoded.join(weekly_to, [\"to\",\"week_start\"], \"left\")\n",
    "    trx_df = df_encoded.join(total_from, [\"from\"], \"left\")\n",
    "    trx_df = df_encoded.join(total_to, [\"to\"], \"left\")\n",
    "    trx_df = df_encoded.join(daily_avg_interval_from, [\"from\",\"date\"], \"left\")\n",
    "    trx_df = df_encoded.join(daily_avg_interval_to, [\"to\",\"date\"], \"left\")\n",
    "    \n",
    "    return trx_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
